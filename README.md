# Visual Communicability and Semantic Correspondence Index (VCSCI). Version 0.1

## Overview

The Visual Communicability and Semantic Correspondence Index (VCSCI) is a methodological framework to evaluate the communicative adequacy of pictograms generated by a machine learning model.

It is based on the principle that pictograms should not only be visually clear but also semantically aligned with communicative intentions. The index is structured to test whether generated pictograms convey meaning accurately, are pragmatically useful, and are culturally and linguistically appropriate.

The repository provides a curated core phrases list in Spanish with British English (NZ) equivalents. These phrases cover multiple communicative functions and daily life domains, enabling a robust evaluation of generative pictogram models.


## Core Phrase List

The core phrases list is divided into separate JSON files by communicative function, following Austin/Searle’s speech act taxonomy.
1. [Requests](core-phrase-list-01-request.json) (Solicitar) ￼
2. [Reject/Protest](core-phrase-list-02-reject.json) (Rechazar) ￼
3. [Direct/Give](core-phrase-list-03-direct.json) Instructions (Dirigir) ￼
4. [Accept/Agree](core-phrase-list-04-accept.json) (Aceptar) ￼
5.  [Social Interaction](core-phrase-list-05-interact.json) (Interacción social) ￼
6.  [Express Feelings/Affection](core-phrase-list-06-express.json) (Expresar afecto/emoción) ￼
7. [Comment/Describe](core-phrase-list-07-comment.json) (Comentar) ￼
8. [Ask Questions](core-phrase-list-08-ask.json) (Preguntar) ￼

Each file contains around 10–15 phrases, totalling approximately 100 core bilingual phrases. 

- **[Download the complete list](core-phrase-list-all.json)**


## Two Modes of Evaluation

The VCSCI framework supports two complementary evaluation approaches:

### 1. Model-Level Evaluation (Legacy Mode)

Evaluate and compare different generative models or pipeline versions based on collective output quality.

**Use when:**

- Comparing different AI models or versions
- Assessing overall pipeline performance
- Benchmarking against standards
- Research and publication

**Workflow:**

1. Generate pictograms for a phrase set
2. Collect evaluation ratings
3. Aggregate scores across all pictograms
4. Compare models/versions

**Input:** Phrase set → **Output:** Aggregated VCSCI score per model

See: [docs/pictogram-case.md](docs/pictogram-case.md#relationship-to-model-level-evaluation)

### 2. Pictogram-Level Evaluation (New Mode)

Evaluate individual pictograms for iterative refinement and production deployment decisions.

**Use when:**

- Deciding if a pictogram is ready for production
- Identifying specific improvements needed
- Iterating on individual pictograms
- Quality control for AAC systems

**Workflow:**

1. Generate individual pictogram cases
2. Evaluate using standardized instrument
3. Get decision (accept/accept-with-edits/reject)
4. Refine based on required edits

**Input:** Single pictogram → **Output:** Individual assessment + decision

See: [docs/evaluation-instrument.md](docs/evaluation-instrument.md)


## Methodology

The evaluation process involves five stages:

1. Definition of the Base Concept Set
   - Establish a representative list of core phrases relevant to Augmentative and Alternative Communication (AAC).
   - Cover all major communicative functions (requests, protests, comments, questions, social interaction, emotion, acceptance).
   - Ensure representation across daily life domains (home, school, health/hygiene, leisure/community).

2. Generative Pictogram Production
   - Generate pictograms in SVG format using a generative model.
   - Iterate systematically, keeping input prompts consistent across phrase categories.

3. Expert and User Evaluation
   - Assess clarity, recognisability, and cultural adequacy of pictograms.
   - Criteria include:
   - Cognitive accessibility (ISO/IEC 24751 compliance)
   - Semantic transparency (does the pictogram convey the intended concept without ambiguity?)
   - Pragmatic fit (is it useful in natural communication contexts?)
   - Performance (average time per generation).

4. Composite Scoring
   - An integrated numerical index is produced, combining cognitive accessibility, semantic correspondence, and cultural adequacy.
   - This becomes the VCSCI score, enabling comparison across iterations.

5. Iteration and Refinement
   - The model is adjusted after each evaluation round.
   - Improvements are tracked via semantic versioning.


## Semantic Versioning

This repository follows semantic versioning (SemVer) to reflect progress in phrase list design, methodology, and pictogram generation.
 - 0.1.0 – Initial release with core phrase lists by communicative function.
 - Future releases will expand the dataset, refine evaluation criteria, and integrate user feedback.


## Merging JSON Files

You can edit JSON files separately and then compile them into a single unified file for evaluation.

```bash
python3 merge-core-phrases.py
```

to produce an updated compiled JSON file.

## Centralized Rubric Descriptions

**New Feature:** All operational definitions for the VCSCI evaluation rubric are now stored in a single source of truth:

**[data/rubric-scale-descriptions.json](data/rubric-scale-descriptions.json)**

This enables:

- **Consistent evaluation** - Same descriptions across all interfaces
- **Programmatic access** - Scripts and web UIs query the same source
- **Bilingual support** - Spanish and English in sync
- **Compiled evaluations** - Scores automatically compile into narrative paragraphs

### Usage Examples

```bash
# Compile evaluation scores into narrative text
node scripts/compile-evaluation-text.js --scores 5,4,3,4,5,4

# Output in HTML format for web display
node scripts/compile-evaluation-text.js --scores 5,4,3,4,5,4 --format html

# English output
node scripts/compile-evaluation-text.js --scores 5,4,3,4,5,4 --lang en
```

**See:** [docs/rubric-descriptions-usage.md](docs/rubric-descriptions-usage.md) for complete documentation.

**Interactive Demo:** Open [examples/hexagonal-rating-with-descriptions.html](examples/hexagonal-rating-with-descriptions.html) to see:

- Real-time rubric descriptions as you rate dimensions (Lexend typography)
- Compiled evaluation (overall) showing all 6 paragraphs
- One-click JSON export with complete rubric transparency
- Hexagonal visualization updating in real-time

**Example Export:** See [examples/exported-evaluation-example.json](examples/exported-evaluation-example.json) for a complete exported evaluation with all rubric descriptions included.

## Quick Start

### For Model-Level Evaluation

```bash
# 1. Generate pictograms (using your model)
node scripts/generate-cases.js --random 20

# 2. Collect evaluations (export from Google Sheets)
node scripts/import-ratings.js ratings-export.csv

# 3. Compute scores
node scripts/score-cases.js

# 4. Aggregate by model/version
node scripts/aggregate-scores.js

# 5. View results
cat analysis/results/pipeline-comparison.json
```

### For Pictogram-Level Evaluation

```bash
# 1. Generate a single pictogram case
node scripts/generate-cases.js --phrases req-001

# 2. Evaluate using the instrument (see docs/evaluation-instrument.md)

# 3. Import rating
node scripts/import-ratings.js single-rating.csv

# 4. Get decision
node scripts/score-cases.js
cat analysis/results/case-scores.json | jq '.["req-001_v1.0.0_default-v1_01"]'

# 5. Iterate if needed (regenerate with edits)
```

## Repository Structure

```text
├── core-phrase-list-*.json    # Phrase lists by function (with phrase_ids)
├── cases/                     # Pictogram case definitions
├── pictograms/                # Generated pictogram outputs
├── ratings/                   # Evaluation records
├── analysis/                  # Results and reports
├── schemas/                   # JSON schemas for validation
├── scripts/                   # Automation scripts
├── style/                     # Style profile configurations
└── docs/                      # Documentation
```

## Documentation

- [Pictogram Case Definition](docs/pictogram-case.md) - What is a pictogram case
- [Generation Workflow](docs/generation-workflow.md) - How to generate cases
- [Evaluation Instrument](docs/evaluation-instrument.md) - How to evaluate
- [Evaluation Rubric](docs/rubric.md) - Detailed rating criteria
- [Rubric Descriptions Usage](docs/rubric-descriptions-usage.md) - How to use centralized rubric JSON
- [Data Schemas](schemas/README.md) - Data structure validation
- [Data Handling](docs/data-handling.md) - Privacy and governance

## Working with Google Spreadsheet

We have a shared spreadsheet for validating, updating and working [here](https://docs.google.com/spreadsheets/d/1gGF8FiW6n2mepbdyasWGiE2wf1b54ufhRWS23dUyiQM/edit?usp=sharing).

You can request editing access.

Go to "Extensions > App Scripts" and use:

- [import script](import.js) imports the current compiled file
- [export script](export.js) exports all JSON files
